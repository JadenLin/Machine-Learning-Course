{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW3 - Stock Movement Prediction\n",
    "\n",
    "作業檔案：\n",
    "- hw3.ipynb\n",
    "\n",
    "資料：\n",
    "https://www.sharecast.com/index/SP_500/prices/download\n",
    "\n",
    "- train.csv: S&P 500 訓練資料(2009-2017)\n",
    "- test.csv: S&P 500 測試資料(2018)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2264, 6)\n",
      "          Date  Open Price  Close Price  High Price  Low Price      Volume\n",
      "0  02-Jan-2009      902.99       931.80      934.73     899.35  4048270080\n",
      "1  05-Jan-2009      929.17       927.45      936.63     919.53  5413910016\n",
      "2  06-Jan-2009      931.17       934.70      943.85     927.28  5392620032\n",
      "3  07-Jan-2009      927.45       906.65      927.45     902.37  4704940032\n",
      "4  08-Jan-2009      905.73       909.73      910.00     896.81  4991549952\n",
      "(252, 6)\n",
      "          Date  Open Price  Close Price  High Price  Low Price      Volume\n",
      "0  02-Jan-2018     2683.73      2695.81     2695.89    2682.36  1846463232\n",
      "1  03-Jan-2018     2697.85      2713.06     2714.37    2697.77  2090595328\n",
      "2  04-Jan-2018     2719.31      2723.99     2729.29    2719.07  2100767744\n",
      "3  05-Jan-2018     2731.33      2743.15     2743.45    2727.92  1918869120\n",
      "4  08-Jan-2018     2742.67      2747.71     2748.51    2737.60  1894823936\n"
     ]
    }
   ],
   "source": [
    "# Read data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train_data_path = './train.csv'\n",
    "test_data_path = './test.csv'\n",
    "\n",
    "train_df = pd.read_csv(train_data_path)\n",
    "test_df = pd.read_csv(test_data_path)\n",
    "\n",
    "print(train_df.shape)\n",
    "print(train_df.head())\n",
    "print(test_df.shape)\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2264, 5)\n",
      "   Open Price  Close Price  High Price  Low Price      Volume\n",
      "0      902.99       931.80      934.73     899.35  4048270080\n",
      "1      929.17       927.45      936.63     919.53  5413910016\n",
      "2      931.17       934.70      943.85     927.28  5392620032\n",
      "3      927.45       906.65      927.45     902.37  4704940032\n",
      "4      905.73       909.73      910.00     896.81  4991549952\n",
      "(252, 5)\n",
      "   Open Price  Close Price  High Price  Low Price      Volume\n",
      "0     2683.73      2695.81     2695.89    2682.36  1846463232\n",
      "1     2697.85      2713.06     2714.37    2697.77  2090595328\n",
      "2     2719.31      2723.99     2729.29    2719.07  2100767744\n",
      "3     2731.33      2743.15     2743.45    2727.92  1918869120\n",
      "4     2742.67      2747.71     2748.51    2737.60  1894823936\n"
     ]
    }
   ],
   "source": [
    "# Drop unnecessary columns\n",
    "\n",
    "drop_col_names = ['Date'] # !--- or you can modify it to drop the columns you don't want ---!\n",
    "\n",
    "train_df.drop(columns=drop_col_names, inplace=True)\n",
    "test_df.drop(columns=drop_col_names, inplace=True)\n",
    "\n",
    "print(train_df.shape)\n",
    "print(train_df.head())\n",
    "print(test_df.shape)\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Open Price  Close Price  High Price  Low Price      Volume  \\\n",
      "0      902.99       931.80      934.73     899.35  4048270080   \n",
      "1      929.17       927.45      936.63     919.53  5413910016   \n",
      "2      931.17       934.70      943.85     927.28  5392620032   \n",
      "3      927.45       906.65      927.45     902.37  4704940032   \n",
      "4      905.73       909.73      910.00     896.81  4991549952   \n",
      "\n",
      "   Tomorrow Movement  \n",
      "0                0.0  \n",
      "1                1.0  \n",
      "2                0.0  \n",
      "3                1.0  \n",
      "4                0.0  \n",
      "      Open Price  Close Price  High Price  Low Price      Volume  \\\n",
      "2259     2684.22      2683.34     2685.35    2678.13  1383888512   \n",
      "2260     2679.09      2680.50     2682.74    2677.96  1103808384   \n",
      "2261     2682.10      2682.62     2685.64    2678.91  1149108352   \n",
      "2262     2686.10      2687.54     2687.66    2682.69  1126089856   \n",
      "2263     2689.15      2673.61     2692.12    2673.61  1332374016   \n",
      "\n",
      "      Tomorrow Movement  \n",
      "2259                0.0  \n",
      "2260                1.0  \n",
      "2261                1.0  \n",
      "2262                0.0  \n",
      "2263                NaN  \n"
     ]
    }
   ],
   "source": [
    "# Add the column `Tomorrow Movement` by comparing the `Close Price` with the previous days as the training target\n",
    "\n",
    "train_df['Tomorrow Movement'] = np.where(train_df['Close Price'].diff() >= 0, 1, 0)\n",
    "test_df['Tomorrow Movement'] = np.where(test_df['Close Price'].diff() >= 0, 1, 0)\n",
    "\n",
    "train_df['Tomorrow Movement'] = train_df['Tomorrow Movement'].shift(-1)\n",
    "test_df['Tomorrow Movement'] = test_df['Tomorrow Movement'].shift(-1)\n",
    "\n",
    "print(train_df.head())\n",
    "print(train_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !--- You can add your own data preprocessing here ---!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2263, 6)\n"
     ]
    }
   ],
   "source": [
    "# Drop rows with NaN values\n",
    "\n",
    "train_df = train_df.dropna()\n",
    "test_df = test_df.dropna()\n",
    "\n",
    "print(train_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2263, 5)\n",
      "   Open Price  Close Price  High Price  Low Price      Volume\n",
      "0      902.99       931.80      934.73     899.35  4048270080\n",
      "1      929.17       927.45      936.63     919.53  5413910016\n",
      "2      931.17       934.70      943.85     927.28  5392620032\n",
      "3      927.45       906.65      927.45     902.37  4704940032\n",
      "4      905.73       909.73      910.00     896.81  4991549952\n",
      "(2263,)\n",
      "0    0.0\n",
      "1    1.0\n",
      "2    0.0\n",
      "3    1.0\n",
      "4    0.0\n",
      "Name: Tomorrow Movement, dtype: float64\n",
      "-----\n",
      "(251, 5)\n",
      "   Open Price  Close Price  High Price  Low Price      Volume\n",
      "0     2683.73      2695.81     2695.89    2682.36  1846463232\n",
      "1     2697.85      2713.06     2714.37    2697.77  2090595328\n",
      "2     2719.31      2723.99     2729.29    2719.07  2100767744\n",
      "3     2731.33      2743.15     2743.45    2727.92  1918869120\n",
      "4     2742.67      2747.71     2748.51    2737.60  1894823936\n",
      "(251,)\n",
      "0    1.0\n",
      "1    1.0\n",
      "2    1.0\n",
      "3    1.0\n",
      "4    1.0\n",
      "Name: Tomorrow Movement, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Divide x and y data\n",
    "\n",
    "train_x_df = train_df.drop(columns=['Tomorrow Movement'])\n",
    "train_y_df = train_df['Tomorrow Movement']\n",
    "\n",
    "test_x_df = test_df.drop(columns=['Tomorrow Movement'])\n",
    "test_y_df = test_df['Tomorrow Movement']\n",
    "\n",
    "print(train_x_df.shape)\n",
    "print(train_x_df.head())\n",
    "print(train_y_df.shape)\n",
    "print(train_y_df.head())\n",
    "print('-----')\n",
    "print(test_x_df.shape)\n",
    "print(test_x_df.head())\n",
    "print(test_y_df.shape)\n",
    "print(test_y_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Open Price  Close Price  High Price  Low Price    Volume\n",
      "0   -1.552572    -1.494607   -1.505683  -1.541181  0.813175\n",
      "1   -1.498571    -1.503581   -1.501760  -1.499581  1.823826\n",
      "2   -1.494446    -1.488625   -1.486853  -1.483605  1.808070\n",
      "3   -1.502119    -1.546489   -1.520714  -1.534956  1.299148\n",
      "4   -1.546921    -1.540136   -1.556744  -1.546417  1.511255\n",
      "0    0.0\n",
      "1    1.0\n",
      "2    0.0\n",
      "3    1.0\n",
      "4    0.0\n",
      "Name: Tomorrow Movement, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Normalize data\n",
    "\n",
    "# !--- Modify here if you want ---!\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_x_df)\n",
    "\n",
    "normalized_train_x_df = scaler.transform(train_x_df)\n",
    "normalized_train_x_df = np.transpose(normalized_train_x_df)\n",
    "\n",
    "normalized_train_x_df = pd.DataFrame({\n",
    "    'Open Price': normalized_train_x_df[0],\n",
    "    'Close Price': normalized_train_x_df[1],\n",
    "    'High Price': normalized_train_x_df[2],\n",
    "    'Low Price': normalized_train_x_df[3],\n",
    "    'Volume': normalized_train_x_df[4],\n",
    "})\n",
    "\n",
    "normalized_test_x_df = scaler.transform(test_x_df)\n",
    "normalized_test_x_df = np.transpose(normalized_test_x_df)\n",
    "normalized_test_x_df = pd.DataFrame({\n",
    "    'Open Price': normalized_test_x_df[0],\n",
    "    'Close Price': normalized_test_x_df[1],\n",
    "    'High Price': normalized_test_x_df[2],\n",
    "    'Low Price': normalized_test_x_df[3],\n",
    "    'Volume': normalized_test_x_df[4],\n",
    "})\n",
    "\n",
    "print(normalized_train_x_df.head())\n",
    "print(train_y_df[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jaden\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\jaden\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\jaden\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\jaden\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\jaden\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\jaden\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\jaden\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\jaden\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\jaden\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "c:\\users\\jaden\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy:\n",
      "0.5470614228899691\n",
      "\n",
      "testing accuracy:\n",
      "0.5418326693227091\n",
      "\n",
      "predicted testing labels:\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 1. 0. 1. 0. 0. 0. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1.\n",
      " 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1.\n",
      " 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 1. 0. 1. 1. 0. 0.\n",
      " 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Train & Predict using Logistic Regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#lr_model = LogisticRegression() # !--- Initialize the model here ---!\n",
    "#lr_model.fit(normalized_train_x_df, train_y_df) # !-- Fill the training data here --!\n",
    "\n",
    "lr_best_acc = 0\n",
    "for i in range(1,10):\n",
    "    lr_model = LogisticRegression(penalty='l1',C=i)\n",
    "    lr_model.fit(normalized_train_x_df, train_y_df)\n",
    "    \n",
    "    lr_training_acc = accuracy_score(train_y_df, lr_model.predict(normalized_train_x_df))\n",
    "    \n",
    "    lr_predict_test_result = lr_model.predict(normalized_test_x_df)\n",
    "    lr_testing_acc = accuracy_score(test_y_df, lr_predict_test_result)\n",
    "    if lr_testing_acc > lr_best_acc:\n",
    "        lr_best_train_acc = lr_training_acc\n",
    "        lr_best_acc = lr_testing_acc\n",
    "\n",
    "print('training accuracy:')\n",
    "# !-- Predict training target & print the training accuracy here --!\n",
    "#lr_training_acc = accuracy_score(train_y_df, lr_model.predict(normalized_train_x_df))\n",
    "print(lr_best_train_acc)\n",
    "\n",
    "print('\\ntesting accuracy:')\n",
    "# !-- Predict testing target & print the testing accuracy here --!\n",
    "#lr_predict_test_result = lr_model.predict(normalized_test_x_df)\n",
    "#lr_testing_acc = accuracy_score(test_y_df, lr_predict_test_result)\n",
    "print(lr_best_acc)\n",
    "\n",
    "print('\\npredicted testing labels:')\n",
    "print(lr_predict_test_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision, recall, fbeta-score:\n",
      "(0.490867040313323, 0.5059760956175299, 0.46845659393221956, None)\n",
      "\n",
      "confusion matrix(tn, fp, fn, tp):\n",
      "(27, 92, 32, 100)\n"
     ]
    }
   ],
   "source": [
    "# Print precision, recall, fbeta-score and confusion matrix\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print('precision, recall, fbeta-score:')\n",
    "print(precision_recall_fscore_support(test_y_df, lr_predict_test_result, average='weighted'))\n",
    "print('\\nconfusion matrix(tn, fp, fn, tp):')\n",
    "tn, fp, fn, tp = confusion_matrix(test_y_df, lr_predict_test_result).ravel()\n",
    "print((tn, fp, fn, tp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy:\n",
      "0.5528060097216085\n",
      "\n",
      "testing accuracy:\n",
      "0.6095617529880478\n",
      "\n",
      "predicted testing labels:\n",
      "[1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0.\n",
      " 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Train & Predict with SVC\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#svc_model = SVC() # !--- Initialize the model here ---!\n",
    "#svc_model.fit(normalized_train_x_df, train_y_df) # !-- Fill the training data here --!\n",
    "\n",
    "svc_best_acc = 0\n",
    "for i in range(1,10):\n",
    "    svc_model = SVC(C=i)\n",
    "    svc_model.fit(normalized_train_x_df, train_y_df)\n",
    "    \n",
    "    svc_training_acc = accuracy_score(train_y_df, svc_model.predict(normalized_train_x_df))\n",
    "    \n",
    "    svc_predict_test_result = svc_model.predict(normalized_test_x_df)\n",
    "    svc_testing_acc = accuracy_score(test_y_df, svc_predict_test_result)\n",
    "    if svc_testing_acc > svc_best_acc:\n",
    "        svc_best_training_acc = svc_training_acc\n",
    "        svc_best_acc = svc_testing_acc\n",
    "\n",
    "print('training accuracy:')\n",
    "# !-- Predict training target & print the training accuracy here --!\n",
    "#svc_training_acc = accuracy_score(train_y_df, svc_model.predict(normalized_train_x_df))\n",
    "print(svc_best_training_acc)\n",
    "\n",
    "print('\\ntesting accuracy:')\n",
    "# !-- Predict testing target & print the testing accuracy here --!\n",
    "#svc_predict_test_result = svc_model.predict(normalized_test_x_df)\n",
    "#svc_testing_acc = accuracy_score(test_y_df, svc_predict_test_result)\n",
    "print(svc_best_acc)\n",
    "\n",
    "print('\\npredicted testing labels:')\n",
    "print(svc_predict_test_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision, recall, fbeta-score:\n",
      "(0.6103152606963451, 0.6055776892430279, 0.60515186218688, None)\n",
      "\n",
      "confusion matrix(tn, fp, fn, tp):\n",
      "(78, 41, 58, 74)\n"
     ]
    }
   ],
   "source": [
    "# Print precision, recall, fbeta-score and confusion matrix\n",
    "\n",
    "print('precision, recall, fbeta-score:')\n",
    "print(precision_recall_fscore_support(test_y_df, svc_predict_test_result, average='weighted'))\n",
    "print('\\nconfusion matrix(tn, fp, fn, tp):')\n",
    "tn, fp, fn, tp = confusion_matrix(test_y_df, svc_predict_test_result).ravel()\n",
    "print((tn, fp, fn, tp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2263, 2)\n",
      "   0  1\n",
      "0  1  0\n",
      "1  0  1\n",
      "2  1  0\n",
      "3  0  1\n",
      "4  1  0\n"
     ]
    }
   ],
   "source": [
    "# Define NN output groundtruth\n",
    "\n",
    "falling_prob = pd.DataFrame(data=np.where(train_y_df == 0, 1, 0)[:])\n",
    "train_y_df = pd.DataFrame(data=np.where(train_y_df == 0, 0, 1)[:])\n",
    "train_y_df = pd.concat( [ falling_prob, train_y_df ], axis=1, ignore_index=True )\n",
    "\n",
    "falling_prob = pd.DataFrame(data=np.where(test_y_df == 0, 1, 0)[:])\n",
    "test_y_df = pd.DataFrame(data=np.where(test_y_df == 0, 0, 1)[:])\n",
    "test_y_df = pd.concat( [ falling_prob, test_y_df ], axis=1, ignore_index=True )\n",
    "\n",
    "print(train_y_df.shape)\n",
    "print(train_y_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 - loss:0.6970199347\n",
      "epoch:100 - loss:0.6867566705\n",
      "epoch:200 - loss:0.6864835620\n",
      "epoch:300 - loss:0.6864428520\n",
      "epoch:400 - loss:0.6864324808\n"
     ]
    }
   ],
   "source": [
    "# Define NN structure\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# !--- You can modify the NN structure here ---!\n",
    "class M_NN(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super(M_NN, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.linear2 = torch.nn.Linear(H, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.linear1(x)\n",
    "        acti_out = F.relu(h)\n",
    "        y_pred = self.linear2(h)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "# N = batch size, D_in = input size, H = hidden size, D_out = output size\n",
    "N, D_in, H, D_out = 100, 5, 70, 2  # !--- You can modify here ---!\n",
    "\n",
    "model = M_NN(D_in, H, D_out)\n",
    "criterion = torch.nn.BCEWithLogitsLoss(reduction='mean') # !--- You can modify here ---!\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=5e-3) # !--- You can modify here ---!\n",
    "\n",
    "\n",
    "# Train NN\n",
    "# !--- You can modify here ---!\n",
    "\n",
    "for t in range(500):\n",
    "    for batch_num in range(N, len(normalized_train_x_df), N): \n",
    "        y_pred = model(torch.FloatTensor(normalized_train_x_df[batch_num-N : batch_num].values.tolist())) # !-- Fill the training batch data here --!\n",
    "        loss = criterion(y_pred, torch.FloatTensor(train_y_df[batch_num-N : batch_num].values.tolist())) # !-- Fill the prediction & groundtruth here to calculate loss --!\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if (t%100 == 0):\n",
    "        print('epoch:%d - loss:%.10f' % (t, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy:\n",
      "0.5475033141847105\n",
      "\n",
      "testing accuracy:\n",
      "0.5258964143426295\n",
      "\n",
      "predicted testing prob:\n",
      "tensor([[-0.0616,  0.0670],\n",
      "        [-0.0564,  0.0639],\n",
      "        [-0.0579,  0.0615],\n",
      "        [-0.0560,  0.0611],\n",
      "        [-0.0582,  0.0598],\n",
      "        [-0.0576,  0.0592],\n",
      "        [-0.0590,  0.0593],\n",
      "        [-0.0508,  0.0577],\n",
      "        [-0.0472,  0.0552],\n",
      "        [-0.0637,  0.0562],\n",
      "        [-0.0446,  0.0532],\n",
      "        [-0.0536,  0.0521],\n",
      "        [-0.0464,  0.0496],\n",
      "        [-0.0395,  0.0508],\n",
      "        [-0.0452,  0.0474],\n",
      "        [-0.0512,  0.0482],\n",
      "        [-0.0504,  0.0471],\n",
      "        [-0.0338,  0.0458],\n",
      "        [-0.0515,  0.0469],\n",
      "        [-0.0523,  0.0492],\n",
      "        [-0.0510,  0.0480],\n",
      "        [-0.0439,  0.0498],\n",
      "        [-0.0764,  0.0584],\n",
      "        [-0.1089,  0.0782],\n",
      "        [-0.0328,  0.0693],\n",
      "        [-0.0620,  0.0683],\n",
      "        [-0.1238,  0.0843],\n",
      "        [-0.0760,  0.0780],\n",
      "        [-0.0617,  0.0729],\n",
      "        [-0.0649,  0.0727],\n",
      "        [-0.0470,  0.0700],\n",
      "        [-0.0597,  0.0642],\n",
      "        [-0.0553,  0.0633],\n",
      "        [-0.0659,  0.0655],\n",
      "        [-0.0695,  0.0693],\n",
      "        [-0.0657,  0.0677],\n",
      "        [-0.0495,  0.0640],\n",
      "        [-0.0473,  0.0576],\n",
      "        [-0.0737,  0.0623],\n",
      "        [-0.0782,  0.0651],\n",
      "        [-0.0845,  0.0714],\n",
      "        [-0.0550,  0.0697],\n",
      "        [-0.0474,  0.0673],\n",
      "        [-0.0658,  0.0625],\n",
      "        [-0.0568,  0.0642],\n",
      "        [-0.0601,  0.0617],\n",
      "        [-0.0424,  0.0581],\n",
      "        [-0.0572,  0.0555],\n",
      "        [-0.0692,  0.0604],\n",
      "        [-0.0707,  0.0613],\n",
      "        [-0.0626,  0.0612],\n",
      "        [-0.0471,  0.0509],\n",
      "        [-0.0812,  0.0671],\n",
      "        [-0.0613,  0.0632],\n",
      "        [-0.0615,  0.0664],\n",
      "        [-0.0922,  0.0748],\n",
      "        [-0.1036,  0.0838],\n",
      "        [-0.0590,  0.0753],\n",
      "        [-0.1036,  0.0817],\n",
      "        [-0.0766,  0.0778],\n",
      "        [-0.0601,  0.0766],\n",
      "        [-0.1096,  0.0863],\n",
      "        [-0.0708,  0.0796],\n",
      "        [-0.0530,  0.0811],\n",
      "        [-0.0681,  0.0716],\n",
      "        [-0.1002,  0.0840],\n",
      "        [-0.0743,  0.0817],\n",
      "        [-0.0623,  0.0728],\n",
      "        [-0.0722,  0.0755],\n",
      "        [-0.0642,  0.0717],\n",
      "        [-0.0837,  0.0746],\n",
      "        [-0.0654,  0.0703],\n",
      "        [-0.0585,  0.0666],\n",
      "        [-0.0640,  0.0645],\n",
      "        [-0.0707,  0.0661],\n",
      "        [-0.0795,  0.0696],\n",
      "        [-0.0738,  0.0715],\n",
      "        [-0.0977,  0.0778],\n",
      "        [-0.0747,  0.0744],\n",
      "        [-0.0609,  0.0699],\n",
      "        [-0.0728,  0.0692],\n",
      "        [-0.0813,  0.0731],\n",
      "        [-0.0702,  0.0724],\n",
      "        [-0.0795,  0.0734],\n",
      "        [-0.0786,  0.0755],\n",
      "        [-0.0567,  0.0773],\n",
      "        [-0.0670,  0.0704],\n",
      "        [-0.0700,  0.0693],\n",
      "        [-0.0582,  0.0677],\n",
      "        [-0.0564,  0.0655],\n",
      "        [-0.0614,  0.0640],\n",
      "        [-0.0634,  0.0636],\n",
      "        [-0.0692,  0.0647],\n",
      "        [-0.0587,  0.0646],\n",
      "        [-0.0629,  0.0647],\n",
      "        [-0.0653,  0.0634],\n",
      "        [-0.0579,  0.0627],\n",
      "        [-0.0680,  0.0633],\n",
      "        [-0.0544,  0.0628],\n",
      "        [-0.0682,  0.0636],\n",
      "        [-0.0657,  0.0646],\n",
      "        [-0.0747,  0.0681],\n",
      "        [-0.0535,  0.0650],\n",
      "        [-0.0679,  0.0618],\n",
      "        [-0.0547,  0.0627],\n",
      "        [-0.0570,  0.0598],\n",
      "        [-0.0602,  0.0598],\n",
      "        [-0.0499,  0.0581],\n",
      "        [-0.0596,  0.0575],\n",
      "        [-0.0522,  0.0580],\n",
      "        [-0.0535,  0.0565],\n",
      "        [-0.0550,  0.0552],\n",
      "        [-0.0585,  0.0547],\n",
      "        [-0.0543,  0.0546],\n",
      "        [-0.0489,  0.0484],\n",
      "        [-0.0550,  0.0576],\n",
      "        [-0.0545,  0.0587],\n",
      "        [-0.0564,  0.0563],\n",
      "        [-0.0686,  0.0602],\n",
      "        [-0.0549,  0.0535],\n",
      "        [-0.0783,  0.0653],\n",
      "        [-0.0603,  0.0623],\n",
      "        [-0.0752,  0.0688],\n",
      "        [-0.0569,  0.0662],\n",
      "        [-0.0631,  0.0640],\n",
      "        [-0.0570,  0.0662],\n",
      "        [-0.0782,  0.0707],\n",
      "        [-0.0596,  0.0642],\n",
      "        [-0.0533,  0.0640],\n",
      "        [-0.0502,  0.0577],\n",
      "        [-0.0531,  0.0555],\n",
      "        [-0.0592,  0.0580],\n",
      "        [-0.0484,  0.0555],\n",
      "        [-0.0529,  0.0548],\n",
      "        [-0.0559,  0.0545],\n",
      "        [-0.0446,  0.0554],\n",
      "        [-0.0502,  0.0516],\n",
      "        [-0.0539,  0.0520],\n",
      "        [-0.0524,  0.0524],\n",
      "        [-0.0504,  0.0538],\n",
      "        [-0.0501,  0.0510],\n",
      "        [-0.0352,  0.0494],\n",
      "        [-0.0430,  0.0463],\n",
      "        [-0.0631,  0.0526],\n",
      "        [-0.0603,  0.0540],\n",
      "        [-0.0450,  0.0502],\n",
      "        [-0.0547,  0.0517],\n",
      "        [-0.0409,  0.0536],\n",
      "        [-0.0452,  0.0498],\n",
      "        [-0.0451,  0.0499],\n",
      "        [-0.0448,  0.0473],\n",
      "        [-0.0474,  0.0483],\n",
      "        [-0.0488,  0.0486],\n",
      "        [-0.0540,  0.0510],\n",
      "        [-0.0570,  0.0536],\n",
      "        [-0.0450,  0.0513],\n",
      "        [-0.0580,  0.0517],\n",
      "        [-0.0432,  0.0502],\n",
      "        [-0.0435,  0.0498],\n",
      "        [-0.0466,  0.0482],\n",
      "        [-0.0436,  0.0470],\n",
      "        [-0.0474,  0.0483],\n",
      "        [-0.0493,  0.0496],\n",
      "        [-0.0416,  0.0477],\n",
      "        [-0.0375,  0.0437],\n",
      "        [-0.0454,  0.0438],\n",
      "        [-0.0364,  0.0430],\n",
      "        [-0.0468,  0.0437],\n",
      "        [-0.0413,  0.0430],\n",
      "        [-0.0440,  0.0428],\n",
      "        [-0.0452,  0.0429],\n",
      "        [-0.0508,  0.0459],\n",
      "        [-0.0428,  0.0470],\n",
      "        [-0.0466,  0.0461],\n",
      "        [-0.0385,  0.0466],\n",
      "        [-0.0433,  0.0436],\n",
      "        [-0.0371,  0.0413],\n",
      "        [-0.0439,  0.0423],\n",
      "        [-0.0513,  0.0453],\n",
      "        [-0.0354,  0.0436],\n",
      "        [-0.0395,  0.0409],\n",
      "        [-0.0317,  0.0382],\n",
      "        [-0.0285,  0.0274],\n",
      "        [-0.0400,  0.0386],\n",
      "        [-0.0415,  0.0395],\n",
      "        [-0.0437,  0.0429],\n",
      "        [-0.0379,  0.0421],\n",
      "        [-0.0363,  0.0396],\n",
      "        [-0.0380,  0.0392],\n",
      "        [-0.0377,  0.0390],\n",
      "        [-0.0403,  0.0395],\n",
      "        [-0.0533,  0.0437],\n",
      "        [-0.0540,  0.0467],\n",
      "        [-0.0437,  0.0457],\n",
      "        [-0.0431,  0.0446],\n",
      "        [-0.0940,  0.0605],\n",
      "        [-0.0812,  0.0649],\n",
      "        [-0.0644,  0.0582],\n",
      "        [-0.0638,  0.0617],\n",
      "        [-0.0362,  0.0575],\n",
      "        [-0.0592,  0.0549],\n",
      "        [-0.0744,  0.0605],\n",
      "        [-0.0575,  0.0591],\n",
      "        [-0.0671,  0.0603],\n",
      "        [-0.0558,  0.0636],\n",
      "        [-0.1054,  0.0750],\n",
      "        [-0.0489,  0.0669],\n",
      "        [-0.0730,  0.0710],\n",
      "        [-0.0958,  0.0800],\n",
      "        [-0.0459,  0.0660],\n",
      "        [-0.0507,  0.0607],\n",
      "        [-0.0499,  0.0598],\n",
      "        [-0.0734,  0.0641],\n",
      "        [-0.0565,  0.0623],\n",
      "        [-0.0521,  0.0603],\n",
      "        [-0.0355,  0.0548],\n",
      "        [-0.0524,  0.0528],\n",
      "        [-0.0628,  0.0551],\n",
      "        [-0.0840,  0.0656],\n",
      "        [-0.0624,  0.0648],\n",
      "        [-0.0825,  0.0684],\n",
      "        [-0.0510,  0.0659],\n",
      "        [-0.0510,  0.0612],\n",
      "        [-0.0853,  0.0691],\n",
      "        [-0.0737,  0.0712],\n",
      "        [-0.0727,  0.0730],\n",
      "        [-0.0781,  0.0806],\n",
      "        [-0.0585,  0.0698],\n",
      "        [-0.0619,  0.0693],\n",
      "        [-0.0419,  0.0656],\n",
      "        [-0.0614,  0.0635],\n",
      "        [-0.0450,  0.0554],\n",
      "        [-0.0530,  0.0528],\n",
      "        [-0.1009,  0.0702],\n",
      "        [-0.1006,  0.0699],\n",
      "        [-0.0591,  0.0676],\n",
      "        [-0.0998,  0.0805],\n",
      "        [-0.0794,  0.0770],\n",
      "        [-0.0874,  0.0768],\n",
      "        [-0.0687,  0.0730],\n",
      "        [-0.0749,  0.0725],\n",
      "        [-0.0894,  0.0784],\n",
      "        [-0.1041,  0.0872],\n",
      "        [-0.0890,  0.0845],\n",
      "        [-0.1012,  0.0931],\n",
      "        [-0.1041,  0.0920],\n",
      "        [-0.1048,  0.0958],\n",
      "        [-0.1327,  0.1158],\n",
      "        [-0.0602,  0.1068],\n",
      "        [-0.0856,  0.0997],\n",
      "        [-0.0975,  0.0957]], grad_fn=<AddmmBackward>)\n",
      "\n",
      "predicted testing labels:\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "\n",
    "nn_predict_train_y = model(torch.FloatTensor(normalized_train_x_df.values.tolist()))# !-- Predict training data here --!\n",
    "result_train = np.where(nn_predict_train_y[:, 0] > nn_predict_train_y[:, 1], 1, 0) # !-- You can modify here --!\n",
    "print('training accuracy:')\n",
    "print(accuracy_score(train_y_df[0], result_train))\n",
    "\n",
    "nn_predict_test_y = model(torch.FloatTensor(normalized_test_x_df.values.tolist())) # !-- Predict training data here --!\n",
    "result_test = np.where(nn_predict_test_y[:, 0] > nn_predict_test_y[:, 1], 1, 0) # !-- You can modify here --!\n",
    "print('\\ntesting accuracy:')\n",
    "print(accuracy_score(test_y_df[0], result_test))\n",
    "\n",
    "print('\\npredicted testing prob:')\n",
    "print(nn_predict_test_y)\n",
    "print('\\npredicted testing labels:')\n",
    "print(result_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "precision, recall, fbeta-score:\n",
      "(0.2765670386184346, 0.5258964143426295, 0.3624977895207681, None)\n",
      "\n",
      "confusion matrix(tn, fp, fn, tp):\n",
      "(132, 0, 119, 0)\n"
     ]
    }
   ],
   "source": [
    "# Print precision, recall, fbeta-score and confusion matrix\n",
    "\n",
    "print('\\nprecision, recall, fbeta-score:')\n",
    "print(precision_recall_fscore_support(test_y_df[0], result_test, average='weighted'))\n",
    "print('\\nconfusion matrix(tn, fp, fn, tp):')\n",
    "tn, fp, fn, tp = confusion_matrix(test_y_df[0], result_test).ravel()\n",
    "print((tn, fp, fn, tp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "在三種方法中(Logistic regression、SVM、Neural network)，我分別調整model內的參數試圖提升其準確率，詳細如下：<br>\n",
    "1.Logistic Regression:<br>\n",
    "利用迴圈去跑C=1~10個別的準確率，並選擇當中test data準確率最高者；調整penalty='l1'。其他參數調整未有好效果，最終accuracy小幅上升至0.542左右。\n",
    "\n",
    "\n",
    "2.SVM:<br>\n",
    "一樣利用迴圈去跑C=1~10個別的準確率，並選擇當中test data準確率最高者。其他參數調整未有好效果，最終accuracy上升至0.61左右，效果較為顯著。\n",
    "\n",
    "\n",
    "3.Neural Network:<br>\n",
    "我嘗試調整了batch size和hidden size的大小、torch.nn.BCEWithLogitsLoss裡reduction改為'mean'、調整torch.optim.SGD裡的learning rate、調整epoch的range。上述方法都無法成功有顯著的提升，最終的accuracy小幅上升至0.526左右。\n",
    "\n",
    "\n",
    "未來可改善的地方：<br>\n",
    "1.想辦法更了解每個model中參數的意義，以方便調整合適於資料屬性的參數<br>\n",
    "2.查找一些與股票預測相關的指數或方法等等，利用增加新的feature去提升預測準確率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
